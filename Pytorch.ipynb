{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REF: https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\DMLAB\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I1225 10:51:55.285141 10788 file_utils.py:40] PyTorch version 1.3.1 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.3.1\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "clear_output()\n",
    "print(\"PyTorch 版本：\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilize a Dataset Class to convert data fitting BERT requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
    "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    " \n",
    "    \n",
    "class OurDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, data, tokenizer):\n",
    "        if isinstance(data,str) and data in ['train','test']:\n",
    "            self.mode = data\n",
    "            self.df = pd.read_csv(data + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "            self.len = len(self.df)\n",
    "            self.label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "            self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        else : \n",
    "            self.mode='predict'\n",
    "            self.df = pd.DataFrame(data,columns=['sentence'])\n",
    "            self.len = len(self.df)\n",
    "            self.label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "            self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.mode == \"test\":\n",
    "            text,label = self.df.iloc[idx, :].values\n",
    "            label_tensor = None\n",
    "            if idx-1 < 0 :\n",
    "                prevText = ''\n",
    "                nextText, nextLabel = self.df.iloc[idx+1, :].values\n",
    "            elif idx+1 == len(self.df):\n",
    "                prevText, prevLabel = self.df.iloc[idx-1, :].values\n",
    "                nextText = ''\n",
    "            else:\n",
    "                prevText, prevLabel = self.df.iloc[idx-1, :].values\n",
    "                nextText, nextLabel = self.df.iloc[idx+1, :].values\n",
    "                \n",
    "        elif self.mode=='predict':\n",
    "            text= self.df.iloc[idx,0]\n",
    "            label_tensor = None  \n",
    "            if idx-1 < 0 :\n",
    "                prevText= ''\n",
    "                nextText= self.df.iloc[idx+1,0]\n",
    "            elif idx+1 == len(self.df):\n",
    "                prevText= self.df.iloc[idx-1,0]\n",
    "                nextText= ''\n",
    "            else:\n",
    "                prevText= self.df.iloc[idx-1,0]\n",
    "                nextText= self.df.iloc[idx+1,0]\n",
    "                \n",
    "        else:\n",
    "            text, label = self.df.iloc[idx, :].values\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id) \n",
    "            if idx-1 < 0 :\n",
    "                prevText = ''\n",
    "                nextText, nextLabel = self.df.iloc[idx+1, :].values   \n",
    "            elif idx+1 == len(self.df):\n",
    "                prevText, prevLabel = self.df.iloc[idx-1, :].values\n",
    "                nextText = ''\n",
    "            else:\n",
    "                prevText, prevLabel = self.df.iloc[idx-1, :].values\n",
    "                nextText, nextLabel = self.df.iloc[idx+1, :].values   \n",
    "            # 將 label 文字也轉換成索引方便轉換成 tensor\n",
    "         \n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        word_pieces += tokens+ [\"[SEP]\"] \n",
    "        len_text = len(word_pieces)\n",
    "        \n",
    "         # 第二個句子的 BERT tokens\n",
    "        prevTokens = self.tokenizer.tokenize(prevText)\n",
    "        nextTokens = self.tokenizer.tokenize(nextText)\n",
    "        word_pieces += prevTokens + nextTokens+ [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_text\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "       \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([0] * len_text + [1] * len_b, dtype=torch.long)\n",
    "\n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "trainset = OurDataset(\"train\" , tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Convered Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original corpus]\n",
      "句子 ：可是定焦也太小气了如果能忍受它缓慢的存储速度的话\n",
      "分類  ：positive\n",
      "\n",
      "--------------------\n",
      "\n",
      "[tensors]\n",
      "tokens_tensor  ：tensor([ 101, 1377, 3221, 2137, 4193,  738, 1922, 2207, 3698,  749, 1963, 3362,\n",
      "        5543, 2556, 1358, 2124, 5353, 2714, 4638, 2100,  996, 6862, 2428, 4638,\n",
      "        6413,  102, 4500, 4638, 3221, 7770, 6862, 6825, 2864, 2339,  868, 4638,\n",
      "        3198,  952, 3301, 1351,  955,  749, 1378, 5314, 2769,  886, 4500,  749,\n",
      "         676, 1921,  102])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1])\n",
      "\n",
      "label_tensor   ：2\n",
      "\n",
      "--------------------\n",
      "\n",
      "[revert tokens_tensors]\n",
      "[CLS]可是定焦也太小气了如果能忍受它缓慢的存储速度的话[SEP]用的是高速连拍工作的时候朋友借了台给我使用了三天[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 選擇第一個樣本\n",
    "sample_idx = 1\n",
    "\n",
    "# 將原始文本拿出做比較\n",
    "text, label = trainset.df.iloc[sample_idx].values\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "print(f\"\"\"[original corpus]\n",
    "句子 ：{text}\n",
    "分類  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[revert tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 `FakeNewsDataset`，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 32\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([32, 64]) \n",
      "tensor([[ 101, 4500, 4638,  ...,    0,    0,    0],\n",
      "        [ 101, 1377, 3221,  ...,    0,    0,    0],\n",
      "        [ 101, 2339,  868,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 5375, 4157,  ...,    0,    0,    0],\n",
      "        [ 101, 3297, 3209,  ...,    0,    0,    0],\n",
      "        [ 101, 2242, 2391,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([32, 64])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([32, 64])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([32])\n",
      "tensor([1, 2, 1, 0, 0, 0, 1, 1, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 1, 2, 1, 2, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Just Visualizing Batch\n",
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 載入一個可以做中文多分類任務的模型，n_class = 3\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": false,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 3,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"pruned_heads\": {},\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "classification acc: 0.29894313034725717\n",
      "\n",
      "整個分類模型的參數量：102269955\n",
      "線性分類器的參數量：2307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            \n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "\n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)\n",
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 162.221, acc: 0.752\n",
      "[epoch 2] loss: 118.816, acc: 0.799\n",
      "[epoch 3] loss: 97.798, acc: 0.802\n",
      "[epoch 4] loss: 87.248, acc: 0.788\n",
      "[epoch 5] loss: 80.846, acc: 0.831\n",
      "[epoch 6] loss: 76.334, acc: 0.840\n",
      "[epoch 7] loss: 71.036, acc: 0.843\n",
      "[epoch 8] loss: 67.368, acc: 0.849\n",
      "Wall time: 10min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 8 \n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'negative', 1: 'neutral', 2: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# 建立測試集。這邊我們可以用跟訓練時不同的 batch_size，看你 GPU 多大\n",
    "testset = OurDataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=256, \n",
    "                        collate_fn=create_mini_batch)\n",
    "\n",
    "# 用分類模型預測測試集\n",
    "predictions = get_predictions(model, testloader)\n",
    "# 用來將預測的 label id 轉回 label 文字\n",
    "\n",
    "index_map = {v: k for k, v in testset.label_map.items()}\n",
    "print(index_map)\n",
    "\n",
    "result = testset.df\n",
    "temp_list = list()\n",
    "for x in predictions.tolist():\n",
    "    temp_list.append(index_map[x])\n",
    "result['predict'] = temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def single_label_f1(mat):\n",
    "    def f1(p,r):\n",
    "        return 2*p*r / (p+r)\n",
    "    print('='*50)\n",
    "    print ('Performance for Each Label')\n",
    "    for i,label in enumerate(['negative','neutral','postive']):\n",
    "        predict_count = sum(mat.T[i])\n",
    "        precision = mat[i][i] / predict_count\n",
    "        ground_truth_count = sum(mat[i])\n",
    "        recall = mat[i][i] / ground_truth_count\n",
    "        print('{:10} | precision :{:.2f}, Recall : {:.2f}, F1 Score : {:.2f}'.format(label,precision,recall,f1(precision,recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ALL:Macro F1-Score 0.6995\n",
      "[[ 40  10   2]\n",
      " [ 50 402  26]\n",
      " [ 12  14  60]]\n",
      "==================================================\n",
      "Performance for Each Label\n",
      "negative   | precision :0.39, Recall : 0.77, F1 Score : 0.52\n",
      "neutral    | precision :0.94, Recall : 0.84, F1 Score : 0.89\n",
      "postive    | precision :0.68, Recall : 0.70, F1 Score : 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "f1 = f1_score(result['sentiment'],result['predict'],average = 'macro')\n",
    "print ('Test ALL:Macro F1-Score %.4f'%f1)\n",
    "mat = confusion_matrix(result['sentiment'],result['predict'])\n",
    "print(mat)\n",
    "single_label_f1(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_f1(mat):\n",
    "    def f1(p,r):\n",
    "        return 2*p*r / (p+r)\n",
    "    p,r = 0,0\n",
    "    for i,label in enumerate(['negative','neutral','postive']):\n",
    "        predict_count = sum(mat.T[i])\n",
    "        p += mat[i][i] / predict_count\n",
    "        ground_truth_count = sum(mat[i])\n",
    "        r += mat[i][i] / ground_truth_count\n",
    "    p /=3\n",
    "    r /=3\n",
    "    print(f1(p,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717677870224492\n"
     ]
    }
   ],
   "source": [
    "my_f1(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['千呼万唤始出来，',\n",
       "  '尼康的APSC小相机终于发布了，',\n",
       "  'COOLPIX A. 你怎么看呢？',\n",
       "  '我看，尼康是挤牙膏挤惯了啊，',\n",
       "  '1，外观既没有V1时尚，',\n",
       "  '也没P7100专业，',\n",
       "  '反而类似P系列。',\n",
       "  '2，CMOS炒冷饭。',\n",
       "  '3，OVF没有任何提示和显示。',\n",
       "  '（除了框框)',\n",
       "  '4，28MM镜头是不错，',\n",
       "  '可是F2.8定焦也太小气了。',\n",
       "  '5，电池坑爹，',\n",
       "  '用D800和V1的电池很难吗？',\n",
       "  '6，考虑到1100美元的定价，',\n",
       "  '富士X100S表示很欢乐。',\n",
       "  '***好处是，',\n",
       "  '可以确定，',\n",
       "  '尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。',\n",
       "  '等2014年年中跌倒1900左右的时候就可以入手了。'],\n",
       " ['进xe之前知道有一部m8.2才万元左右，',\n",
       "  '十分心动。',\n",
       "  '都拿在手中了才突然醒悟，',\n",
       "  '色彩fuji未必比leica差，',\n",
       "  '结实耐用在数码时代已经不那么重要了，',\n",
       "  '所以拿xe,',\n",
       "  '还能多买只老头玩。'],\n",
       " ['二选一：',\n",
       "  '富士X-Pro1+18mmF2和徕卡X？',\n",
       "  '玩过的小机器里面感觉成像最好的还是适马的DP系列，',\n",
       "  '可是它的机械性能又实在是不好，',\n",
       "  '对焦叽叽的叫声让你觉得随时要坏。',\n",
       "  'NEX5C玩了半年就出掉了主要是边缘成像太差色散严重，',\n",
       "  'NEX5C玩了半年就出掉了主要是边缘成像太差色散严重，',\n",
       "  '用转接要好很多但又没有了自动对焦。',\n",
       "  '徕卡X2出来后我是成都第一批购买的。',\n",
       "  '它的做工确实精湛镜头也很漂亮就是一件工艺品，',\n",
       "  '它的做工确实精湛镜头也很漂亮就是一件工艺品，',\n",
       "  '在实际使用中发现暗光近景不错有一些独特的味道，',\n",
       "  '但是阳光下的远景分辨率说句不好听的就像手机拍的一样而且紫边也不小，',\n",
       "  '但是阳光下的远景分辨率说句不好听的就像手机拍的一样而且紫边也不小，',\n",
       "  '当然徕卡是神物用的不好主要还是我的水平差。',\n",
       "  '出掉x2已经是11月了成都天气一直不太好DP这阳光机也暂时没用。',\n",
       "  '12月陪朋友去买尼康D800时看到了富士的XE1就叫老板拿来玩一下，',\n",
       "  '结果大吃一惊它的高感在1600时居然和我的D700差不多，',\n",
       "  '因为吃过X2的亏又拿到店外试了一下远景不错加了个35/1.4立马拿下现在使用了一段时间非常满意。',\n",
       "  '关于X2在多说两句，',\n",
       "  '按说徕卡的镜头毋庸置疑那块索尼1600万像素传感器也很不错，',\n",
       "  '窃以为还是徕卡的软件功底差了。']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "data = []\n",
    "with open(\"Test_sample.tsv\" , encoding= 'utf-8') as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for i, line in enumerate(tsvreader):\n",
    "            data.append(line)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = open('result.tsv','w',encoding='utf-8')\n",
    "for d in data:\n",
    "    predictset = OurDataset(d, tokenizer=tokenizer)\n",
    "    predictloader = DataLoader(predictset, batch_size=256, \n",
    "                        collate_fn=create_mini_batch)\n",
    "\n",
    "    predictions = get_predictions(model, predictloader)\n",
    "\n",
    "    index_map = {v: k for k, v in predictset.label_map.items()}\n",
    "    \n",
    "    if True:\n",
    "        raw = predictset.df.values.flatten()\n",
    "        f.write('\\t'.join(raw))\n",
    "        f.write('\\n')\n",
    "    \n",
    "    result = [index_map[x] for x in predictions.tolist()]\n",
    "    f.write('\\t'.join(result))\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
